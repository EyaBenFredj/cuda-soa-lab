from fastapi import FastAPI, UploadFile, File, HTTPException
import numpy as np
import time
import io
from prometheus_client import make_asgi_app, Counter, Histogram, Gauge
import os
import sys

# Initialize FastAPI app
app = FastAPI(
    title="GPU Matrix Addition Microservice",
    description="SOA Lab - GPU-accelerated matrix addition service",
    version="1.0.0"
)

# Prometheus metrics
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)

# Custom metrics
REQUEST_COUNT = Counter('request_count', 'Total request count', ['method', 'endpoint'])
REQUEST_LATENCY = Histogram('request_latency_seconds', 'Request latency in seconds', ['endpoint'])
GPU_MEMORY_USAGE = Gauge('gpu_memory_usage_mb', 'GPU Memory Usage in MB')
MATRIX_SIZE = Gauge('matrix_size_elements', 'Number of elements in processed matrices')

# Check if we're on Windows
IS_WINDOWS = os.name == 'nt'

# Try to import GPU libraries (will fail on Windows but that's OK)
GPU_AVAILABLE = False
try:
    from numba import cuda
    GPU_AVAILABLE = cuda.is_available()
    
    # CUDA kernel for matrix addition
    @cuda.jit
    def matrix_add_kernel(a, b, c):
        i, j = cuda.grid(2)
        if i < c.shape[0] and j < c.shape[1]:
            c[i, j] = a[i, j] + b[i, j]
            
    def gpu_matrix_add(matrix_a, matrix_b):
        """Perform matrix addition on GPU"""
        if matrix_a.shape != matrix_b.shape:
            raise ValueError("Matrix shapes must match")
        
        rows, cols = matrix_a.shape
        
        # Copy matrices to GPU
        d_matrix_a = cuda.to_device(matrix_a)
        d_matrix_b = cuda.to_device(matrix_b)
        d_result = cuda.device_array_like(matrix_a)
        
        # Configure kernel launch
        threads_per_block = (16, 16)
        blocks_per_grid_x = (rows + threads_per_block[0] - 1) // threads_per_block[0]
        blocks_per_grid_y = (cols + threads_per_block[1] - 1) // threads_per_block[1]
        blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)
        
        # Launch kernel
        matrix_add_kernel[blocks_per_grid, threads_per_block](d_matrix_a, d_matrix_b, d_result)
        cuda.synchronize()
        
        return d_result.copy_to_host()
        
except ImportError:
    # Fallback to CPU implementation
    print("âš ï¸  GPU libraries not available - running in CPU mode")
    
    def gpu_matrix_add(matrix_a, matrix_b):
        """CPU fallback for matrix addition"""
        return matrix_a + matrix_b

def get_gpu_memory_info():
    """Get GPU memory info - mock for Windows"""
    if IS_WINDOWS:
        return 512, 4096  # Mock values for Windows
    else:
        # On Linux, try to get real GPU info
        try:
            import subprocess
            result = subprocess.check_output([
                'nvidia-smi', '--query-gpu=memory.used,memory.total',
                '--format=csv,noheader,nounits'
            ], text=True)
            used, total = map(int, result.strip().split(', '))
            return used, total
        except:
            return 1024, 8192  # Fallback mock values

@app.get("/")
async def root():
    return {
        "message": "GPU Matrix Addition Microservice",
        "version": "1.0.0",
        "gpu_available": GPU_AVAILABLE,
        "environment": "windows" if IS_WINDOWS else "linux",
        "endpoints": {
            "health": "/health",
            "add": "/add",
            "metrics": "/metrics"
        }
    }

@app.get("/health")
async def health_check():
    REQUEST_COUNT.labels(method='GET', endpoint='/health').inc()
    
    used, total = get_gpu_memory_info()
    
    return {
        "status": "ok",
        "gpu_available": GPU_AVAILABLE,
        "environment": "windows" if IS_WINDOWS else "linux",
        "gpu_info": {
            "memory_used_mb": used,
            "memory_total_mb": total,
            "memory_usage_percent": (used / total) * 100
        }
    }

@app.post("/add")
async def matrix_add(
    file_a: UploadFile = File(..., description="First matrix in .npz format"),
    file_b: UploadFile = File(..., description="Second matrix in .npz format")
):
    REQUEST_COUNT.labels(method='POST', endpoint='/add').inc()
    start_time = time.perf_counter()
    
    # Validate file types
    if not file_a.filename.endswith('.npz') or not file_b.filename.endswith('.npz'):
        raise HTTPException(status_code=400, detail="Both files must be in .npz format")
    
    try:
        # Read uploaded files
        content_a = await file_a.read()
        content_b = await file_b.read()
        
        # Load matrices from NPZ files
        with io.BytesIO(content_a) as buffer_a, io.BytesIO(content_b) as buffer_b:
            matrix_a_data = np.load(buffer_a)
            matrix_b_data = np.load(buffer_b)
            
            # Extract arrays (handle different NPZ structures)
            if 'arr_0' in matrix_a_data:
                matrix_a = matrix_a_data['arr_0']
                matrix_b = matrix_b_data['arr_0']
            else:
                # Get first array in file
                matrix_a = matrix_a_data[matrix_a_data.files[0]]
                matrix_b = matrix_b_data[matrix_b_data.files[0]]
        
        # Ensure matrices are float32
        matrix_a = matrix_a.astype(np.float32)
        matrix_b = matrix_b.astype(np.float32)
        
        # Validate shapes
        if matrix_a.shape != matrix_b.shape:
            raise HTTPException(
                status_code=400,
                detail=f"Matrix shapes must match. Got {matrix_a.shape} and {matrix_b.shape}"
            )
        
        rows, cols = matrix_a.shape
        MATRIX_SIZE.set(rows * cols)
        
        # Perform matrix addition
        gpu_start_time = time.perf_counter()
        result_matrix = gpu_matrix_add(matrix_a, matrix_b)
        gpu_time = time.perf_counter() - gpu_start_time
        
        total_time = time.perf_counter() - start_time
        
        # Update metrics
        REQUEST_LATENCY.labels(endpoint='/add').observe(total_time)
        
        used_memory, total_memory = get_gpu_memory_info()
        GPU_MEMORY_USAGE.set(used_memory)
        
        return {
            "matrix_shape": [int(rows), int(cols)],
            "elapsed_time": round(total_time, 6),
            "gpu_computation_time": round(gpu_time, 6),
            "device": "GPU" if GPU_AVAILABLE else "CPU",
            "gpu_memory_used_mb": used_memory,
            "gpu_memory_total_mb": total_memory,
            "environment": "windows" if IS_WINDOWS else "linux"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing matrices: {str(e)}")
    
    # Add this function to your main.py
def get_gpu_metrics():
    """Get GPU metrics for Prometheus"""
    try:
        import subprocess
        # This will work on the instructor's server with nvidia-smi
        result = subprocess.check_output([
            'nvidia-smi', 
            '--query-gpu=memory.used,memory.total,utilization.gpu,temperature.gpu',
            '--format=csv,noheader,nounits'
        ], text=True)
        
        # Parse the result
        used_memory, total_memory, utilization, temperature = map(float, result.strip().split(', '))
        
        return {
            'gpu_memory_used_mb': used_memory,
            'gpu_memory_total_mb': total_memory,
            'gpu_utilization_percent': utilization,
            'gpu_temperature_c': temperature
        }
    except:
        # Fallback for Windows/local development
        return {
            'gpu_memory_used_mb': 512,
            'gpu_memory_total_mb': 4096,
            'gpu_utilization_percent': 0,
            'gpu_temperature_c': 35
        }

if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", "8000"))
    
    print("ðŸš€ Starting GPU Matrix Addition Service")
    print("=" * 50)
    print(f"ðŸ“ Port: {port}")
    print(f"ðŸŽ® GPU Available: {GPU_AVAILABLE}")
    print(f"ðŸ’» Environment: {'Windows' if IS_WINDOWS else 'Linux'}")
    print(f"ðŸ“Š Metrics: http://localhost:{port}/metrics")
    print(f"â¤ï¸  Health: http://localhost:{port}/health")
    print("=" * 50)
    
    uvicorn.run(app, host="0.0.0.0", port=port, log_level="info")